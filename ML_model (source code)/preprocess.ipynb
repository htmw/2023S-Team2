{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define directories for images\n",
    "source_dir = \"D:/Depression/images/original/train/\"\n",
    "train_dir = \"D:/Depression/images/train/\"\n",
    "test_dir = \"D:/Depression/images/test/\"\n",
    "\n",
    "# Define categories (emotional types)\n",
    "categories = ['depressed','happy','neutral','angry','disgust','fear','surprise']\n",
    "\n",
    "# Define train/test split ratio (e.g. 80% train, 20% test)\n",
    "test_size = 0.3\n",
    "\n",
    "def distribute(source_dir):\n",
    "    # Create train/test directories if they don't exist\n",
    "    if not os.path.exists(train_dir):\n",
    "        os.makedirs(train_dir)\n",
    "    if not os.path.exists(test_dir):\n",
    "        os.makedirs(test_dir)\n",
    "\n",
    "    # Iterate through categories and move images to train/test directories\n",
    "    for category in tqdm(categories):\n",
    "        category_path = os.path.join(source_dir, category)\n",
    "        images = os.listdir(category_path)\n",
    "        train_images, test_images = train_test_split(images, test_size=test_size, random_state=42)\n",
    "    \n",
    "        # Move train images to train directory\n",
    "        train_category_path = os.path.join(train_dir, category)\n",
    "        if not os.path.exists(train_category_path):\n",
    "            os.makedirs(train_category_path)\n",
    "        for image in train_images:\n",
    "            source_path = os.path.join(category_path, image)\n",
    "            target_path = os.path.join(train_category_path, image)\n",
    "            shutil.copy(source_path, target_path)\n",
    "    \n",
    "        # Move test images to test directory\n",
    "        test_category_path = os.path.join(test_dir, category)\n",
    "        if not os.path.exists(test_category_path):\n",
    "            os.makedirs(test_category_path)\n",
    "        for image in test_images:\n",
    "            source_path = os.path.join(category_path, image)\n",
    "            target_path = os.path.join(test_category_path, image)\n",
    "            shutil.copy(source_path, target_path)\n",
    "\n",
    "distribute(source_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 38.46it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "#Resize\n",
    "\n",
    "# Set directory paths\n",
    "train_dir = \"D:/Depression/images/train/\"\n",
    "test_dir = \"D:/Depression/images/test/\"\n",
    "my_dir = \"D:/Depression/images/me_test/\"\n",
    "source_dir = \"D:/Depression/images/3classes/\"\n",
    "omkar = \"D:/Depression/images/omkar/\"\n",
    "\n",
    "def resize(path):\n",
    "    # Set target size\n",
    "    target_size = (100, 100)\n",
    "\n",
    "    # Loop through train directory and resize images\n",
    "    for emo_type in tqdm(os.listdir(path)):\n",
    "        emo_path = os.path.join(path, emo_type)\n",
    "        for img_file in os.listdir(emo_path):\n",
    "            img_path = os.path.join(emo_path, img_file)\n",
    "            img = Image.open(img_path)\n",
    "            img_resized = img.resize(target_size)\n",
    "            img_resized.save(img_path)\n",
    "\n",
    "#resize(train_dir)\n",
    "#resize(test_dir)\n",
    "#resize(my_dir)\n",
    "#resize(source_dir)\n",
    "resize(omkar)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "# Define the coordinates to crop the images\n",
    "left = 0\n",
    "top = 214\n",
    "right = 1279\n",
    "bottom = 1493\n",
    "my_dir = \"D:/Depression/images/me_test/\"\n",
    "\n",
    "# 1279x1706\n",
    "\n",
    "\n",
    "def crop(path):\n",
    "    # Loop through the train dataset and crop the images\n",
    "    for emotion_type in tqdm(os.listdir(path)):\n",
    "        emotion_type_dir = os.path.join(path, emotion_type)\n",
    "        for img_file in os.listdir(emotion_type_dir):\n",
    "            img_path = os.path.join(emotion_type_dir, img_file)\n",
    "            with Image.open(img_path) as img:\n",
    "                cropped_img = img.crop((left, top, right, bottom))\n",
    "                cropped_img.save(img_path)\n",
    "\n",
    "        \n",
    "crop(my_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 90.90it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "# grayscaling\n",
    "\n",
    "def grayscale_images(path):\n",
    "    # Convert images in train folder to grayscale\n",
    "    for foldername in tqdm(os.listdir(path)):\n",
    "        folderpath = os.path.join(path, foldername)\n",
    "        for filename in os.listdir(folderpath):\n",
    "            filepath = os.path.join(folderpath, filename)\n",
    "            with Image.open(filepath).convert('L') as img:\n",
    "                img.save(filepath)\n",
    "\n",
    "train_dir = \"D:/Depression/images/train/\"\n",
    "test_dir = \"D:/Depression/images/test/\"\n",
    "\n",
    "#grayscale_images(train_dir)\n",
    "#grayscale_images(test_dir)\n",
    "#grayscale_images(my_dir)\n",
    "grayscale_images(omkar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def crop_faces(path):\n",
    "    # Load the face detection algorithm\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    \n",
    "    for emo_type in tqdm(os.listdir(path)):\n",
    "        folderpath = os.path.join(path, emo_type)\n",
    "        for image_name in os.listdir(folderpath):\n",
    "            image_path = os.path.join(folderpath, image_name)\n",
    "\n",
    "            image = cv2.imread(image_path)\n",
    "        \n",
    "            # Convert the image to grayscale\n",
    "            gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # Detect the faces in the image\n",
    "            faces = face_cascade.detectMultiScale(gray_image, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "            # Crop and replace the faces\n",
    "            for (x, y, w, h) in faces:\n",
    "                cropped_image = image[y:y+h, x:x+w]\n",
    "                cv2.imwrite(image_path, cropped_image)\n",
    "\n",
    "\n",
    "source_dir = \"D:/Depression/images/original/train/\"\n",
    "crop_faces(source_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "91e161404d9de62de272df7f0415f5dbce3229d319fb588a633f6b18aa442687"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
